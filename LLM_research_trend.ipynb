{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # 운영체제와 상호작용 할 수 있는 방법을 제공하는 모델\n",
    "from dotenv import load_dotenv # env파일 안에있는 API키를 가져오기 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # API키 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain을 통해 openai의 gpt모델 사용\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Applications/anaconda3/envs/AI_2/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf파일을 불러오기 위한 PyPDFLoader를 import\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf파일 로드\n",
    "loader = PyPDFLoader(\"초거대 언어모델 연구 동향 (1).pdf\")\n",
    "\n",
    "# 페이지별 문서 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 청크로 나누기\n",
    "\n",
    "# 방법1\n",
    "\n",
    "# 청킹을 위한 라이브러리 imoprt\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\", # /n/n 기준으로 분할\n",
    "    chunk_size=100,   # 청크 크기는 100자\n",
    "    chunk_overlap=10, # 청크 간 중복된 문자는 10개\n",
    "    length_function=len,  # len 함수로 텍스트의 길이 계산(문자 수 반환)\n",
    "    is_separator_regex=False,  # 구분자로 정규식을 사용하지 않는다\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='10 특집원고  초거대 언어모델 연구 동향\n",
      "치하는 정보들을 기억하지 못하는 장기 의존성 문제\n",
      "가 존재한다. 이러한 문제를 극복하기 위해, LSTM \n",
      "(Long Short-Term Memory) [14] 과 GRU (Gated \n",
      "Recurrent Unit) [15] 가 등장했다. 하지만, 이들은 모두 \n",
      "텍스트에 존재하는 단방향 문맥 정보만 활용한다는 \n",
      "한계를 지닌다.\n",
      "양방향 문맥 정보를 활용하기 위해, ELMo [16] 는 \n",
      "주어진 텍스트에 존재하는 순방향 문맥 정보와 역방\n",
      "향 문맥 정보를 함께 활용하는 양방향 학습을 제안했\n",
      "다. 이를 위해, ELMo 는 순방 향 LSTM과 역방향 \n",
      "LSTM를 동시에 활용한다. 하지만, 이는 LSTM을 기\n",
      "반으로 하기 때문에, LSTM이 지니는 다음과 같은 한\n",
      "계를 그대로 가진다: 1) 하나의 벡터에 텍스트의 모든 \n",
      "정보를 담기 때문에 정보 손실이 발생하고, 2) 입력 \n",
      "텍스트의 길이가 길어지면 기울기 소실 (gradient \n",
      "vanishing)이 발생한다.\n",
      "이러한 한계를 해결하기 위해 나온 것이 바로 \n",
      "Attention Mechanism [2] 과 이를 활용한 Transformer \n",
      "Architecture [3] 이다. Attention Mechanism 은 하나의 \n",
      "벡터에 텍스트의 모든 정보를 담는 RNN, LSTM, \n",
      "GRU와 다르게, 텍스트 내 단어들의 벡터들을 필요에 \n",
      "따라 적절히 활용하는 메커니즘이다. 현재 언어모델\n",
      "의 근간이 되는 Transformer가 바로 이러한 Atten- \n",
      "tion Mechanism을 기반으로 한다. Transformer는 크게 \n",
      "인코더와 디코더로 구성되는데, 인코더는 주어진 텍\n",
      "스트를 이해하는 역할을 하고 디코더는 이해한 텍스\n",
      "트를 기반으로 언어를 생성해내는 역할을 수행한다. \n",
      "이러한 Transformer의 인코더를 기반으로 발전한 대표\n",
      "적인 모델이 Google2)의 BERT (Bidirectional Encoder \n",
      "Representations from Transformers) [17] 이고, 디코더\n",
      "를 기반으로 발전한 대표적인 모델이 OpenAI3)의 \n",
      "GPT (Generative Pretrained Transformer) [18] 이다.\n",
      "BERT는 입력 텍스트의 약 15%에 해당하는 임의의 \n",
      "토큰을 마스킹하고 마스킹된 토큰이 무엇인지 예측하\n",
      "는 MLM (Masked Language Modeling) 방식으로 학습\n",
      "된다. 한편, GPT 는 이전 텍스트를 기반으로 다음에 \n",
      "나올 토큰이 무엇인지 예측하는 NTP (Next Token \n",
      "Prediction) 방식으로 학습된다. 이들은 별도의 레이블\n",
      "링 작업 없이 텍스트 데이터만 있으면 학습을 할 수 \n",
      "있다는 강점을 가진다. 이러한 강점을 바탕으로, 이후 \n",
      "문맥기반 언어모델들은 대용량의 텍스트 데이터로 사\n",
      "전학습 (Pretraining) 하고, 이후 특정 태스크로 미세조\n",
      "2) https://www.google.com/\n",
      "3) https://openai.com/\n",
      "정(Fine-tuning)하는 Pretrain-Finetune 패러다임을 중심\n",
      "으로 발전한다.\n",
      "초거대 언어모델 연구 문맥기반 언어모델 이후, 다\n",
      "양한 연구들에서 모델 및 학습 데이터의 크기와 모델\n",
      "의 성능은 긍정적인 상관 관계를 보인다는 ‘scaling \n",
      "l a w ’  [ 4 ,  1 9 ,  2 0 ]가 밝혀지면서, 초거대 언어모델 \n",
      "(Large Language Model, LLM) 이 등장하기 시작했다. \n",
      "LLM은 기존 언어모델에서와 다르게, 모델의 가중치 \n",
      "업데이트 없이도 새로운 태스크를 수행할 수 있는 \n",
      "In-context learning (Zero-shot learning [21]과 Few-shot \n",
      "learning [22]) 능력을 가진다. 이처럼 작은 크기의 모\n",
      "델에서는 발현되지 않던 LLM의 능력을 창발 능력 \n",
      "(Emergent ability) [23] 이라고 부른다.\n",
      "이러한 LLM의 창발 능력을 잘 이끌어내기 위한 연\n",
      "구 분야가 바로 프롬프트 엔지니어링이다. 프롬프트 \n",
      "엔지니어링이란 LLM이 모델 가중치 업데이트 없이 \n",
      "특정 태스크를 더욱 잘 해결하게 하기 위해, 입력으로 \n",
      "주는 프롬프트를 어떻게 설계할 것인지에 대한 연구 \n",
      "분야이다. 가장 대표적인 프롬프트 엔지니어링 연구\n",
      "는 Chain-of-Thought (CoT) [24] 가 있다. 이는 해결하\n",
      "고자 하는 태스크의 예시를 일련의 중간 추론 단계와 \n",
      "함께 넣어줌으로써, 복잡한 문제를 여러 단계로 나누\n",
      "어 해결하는 CoT 프롬프트를 제안했다. 또한, 이러한 \n",
      "프롬프트 엔지니어링까지도 LLM으로 대체하고자 하\n",
      "는 연구도 활발히 진행되고 있다 [25].\n",
      "한편, LLM 의 조종성 (Steerability)을 높이기 위해, \n",
      "Instruction Tuning [26]과 Reinforcement Learning from \n",
      "Human Feedback (RLHF) [27] 과 같은 학습 기법이 등\n",
      "장했다. Instruction Tuning 은 다양한 태스크를 (지시, \n",
      "입력, 출력) 형태의 데이터로 구성하여, 해당 데이터\n",
      "를 통해 LLM을 미세조정하는 학습 기법이다. RLHF\n",
      "는 LLM이 생성할 수 있는 다양한 답변들 중 사용자\n",
      "가 선호할만한 답변을 출력하도록 LLM을 학습하는 \n",
      "기법이다. 하지만, 사용자 선호도를 학습하기 위해 강\n",
      "화학습을 사용하는 RLHF는 학습 과정이 복잡하다는 \n",
      "한계가 있어서, 이를 완화하기 위한 연구도 활발히 진\n",
      "행되고 있다 [28].\n",
      "3. 한국어 초거대 언어모델 동향\n",
      "GPT [21, 22, 29], PALM [30, 20] 과 같은 대규모 \n",
      "LLM 뿐만 아니라,  F a l c o n  [ 3 1 ,  3 2 ] ,  L l a m a  [ 3 3 ,  3 4 ] ,  \n",
      "Claude [35], Qwen [36] 과 같은 비교적 작은 크기의 \n",
      "오픈소스 LLM이 전 세계적으로 공개되고 활발히 연\n",
      "구되고 있다. 하지만, 이러한 LLM들은 일반적으로 한' metadata={'source': '초거대 언어모델 연구 동향 (1).pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "print(splits[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter\n",
    "주어진 텍스트를 기준으로 정의된 구분자를 사용하여 텍스트를 나눈다. 특정 문자를 기준으로 분할해서 문장이나 문단 단위로 텍스트를 나누는데 효과적(간단한 작업, 빠르고 직관적인 경우 유리)\n",
    ">\n",
    "- separator\n",
    "분할된 각 청크를 구분할 때 기준이 되는 문자열\n",
    ">\n",
    "- chunk_size\n",
    "각 청크의 최대 길이\n",
    ">\n",
    "- chunk_overlap\n",
    "인접한 청크 사이에 중복으로 포함될 문자의 수\n",
    ">\n",
    "- length_function\n",
    "청크의 길이를 계산하는 함수\n",
    ">\n",
    "- is_separator_regex\n",
    "매개변수를 False로 설정하여 separator를 정규식이 아닌 일반 문자열로 처리\n",
    ">\n",
    "#### 각 청크의 크기가 chunk_size를 초과하지 않으며 인접한 청크 사이에는 chunk_overlap만큼의 문자가 중복되어 있게 함으로써, 텍스트의 의미적 연속성을 유지하면서 큰 데이터를 더 작은 단위로 분할할 수 있다\n",
    ">\n",
    "- text_splitter \n",
    ">\n",
    "1. split_documents\n",
    "문서(document) 파일을 load한 후 다시 작은 단위조각의 문서(document) list로 반환\n",
    ">\n",
    "2. split_text\n",
    "문자 스트링(str)을 list로 반환\n",
    ">\n",
    "3. create_documents\n",
    "문자열list를 문서(document)list로 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법2\n",
    "\n",
    "# 청킹을 위한 라이브러리 imoprt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,             # 청크 크기는 100자\n",
    "    chunk_overlap=10,           # 청크 간의 중복되는 문자는 10개\n",
    "    length_function=len,        # len함수로 텍스트의 길이 계산\n",
    "    is_separator_regex=False,   # 구분자로 정규식을 사용하지 않는다\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의' metadata={'source': '초거대 언어모델 연구 동향 (1).pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(splits[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter\n",
    "여러 레벨의 구분자를 정의하여 점진적으로 텍스트를 나누는 방식(재귀적으로 분할). e.g. 문단으로 나눈 뒤 문단이 길면 문장 단위로, 문장이 여전히 길면 단어 단위로 나눔(긴 텍스트, 문맥 유지, 복잡한 텍스트 구조에 유리)\n",
    ">\n",
    "- chunk_size\n",
    "청크 크기\n",
    ">\n",
    "- chunk_overlap\n",
    "청크 간의 중복되는 문자 수\n",
    ">\n",
    "- length_function\n",
    "문자열 길이를 계산하는 함수 지정\n",
    ">\n",
    "- is_separator_regex\n",
    "구분자로 정규식을 사용할지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
